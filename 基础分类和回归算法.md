# 基础分类和回归算法

## 1.监督学习

**监督学习**：我们给算法一个数据集，其中包含了正确答案。算法的目的是给出更多的正确答案。

监督学习一般分为**回归问题(预测连续值输出)**和**分类问题(预测离散值输出)**。

## 2.无监督学习

**无监督学习**：一种学习机制，给算法大量的无标签数据，要求其找出数据的类型结构。

**聚类算法**(无监督学习的一种)：将数据集分成很多簇。

## 3.一元线性回归

### 3.1模型描述

![1.png](基础分类和回归算法\1.png)

$$
h(x)=\theta_0+\theta_1x
$$

### 3.2代价函数

假设有$m$个样本$x^{(1)},x^{(2)},......x^{(m)}$
$$
J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^2
$$

$$
\min_{\theta_0,\theta_1} J({\theta_0},\theta_1)
$$

### 3.3用梯度下降法最小化$J({\theta_0},\theta_1)$

梯度下降更新公式
$$
\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J({\theta_0},\theta_1)
$$

$\alpha$称为学习率。$\theta_0$和$\theta_1$要同步更新。

## 4.多元线性回归

假设函数为$h(x_1,x_2...x_n)=\theta_0+\theta_1x_1+\theta_2x_2+...\theta_nx_n$

其矩阵形式为$h(x)=\theta^Tx$，$\theta$是参数向量$(\theta_0,\theta_1...\theta_n)^T$，$x$是特征向量$(x_0,x_1...x_n)^T$，其中$x_0=1$。拓展$x$的维数是为了更简洁的表示$h$。

代价函数为$J(\theta_0,\theta_1,...\theta_n)=\frac{1}{2m}\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^2$，$x^{(i)}$是拓展后的$n+1$维特征列向量。

然后用梯度下降法求使得$J$最小的$(\theta_0,\theta_1...\theta_n)$。梯度下降更新公式：
$$
\begin{eqnarray}
\theta_j &:=& \theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta) \\
              &:=& \theta_j-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j \nonumber
\end{eqnarray}
$$

## 5.梯度下降法技巧—特征缩放

尽量保持每个特征值在相似的范围内。
$$
x:=\frac{x-\mu}{s}
$$
$\mu$是$x$的平均值，$s$是$x$的取值范围。上式称为均值归一化。

## 6.梯度下降法技巧—学习率

学习率太小：收敛缓慢

学习率太大：$J(\theta)$可能在迭代过程中不会减少。

推荐的选取学习率的方式：0.001，0.003，0.01，0.03......这样每隔3倍取一个数字。选取的$\alpha$越大越好，当然要在**$J(\theta)$随着迭代在不断减少**这个前提下。

## 7.特征和多项式回归

在选择特征上我们有很大的自由度，可以选择$(x_1,x_2)$作为特征，也可以选择$x_1*x_2$作为特征。

有时候可以选择一些很复杂的函数来拟合数据，比如多项式拟合。
$$
h(x)=\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3
$$
我们可以设三个新的特征$x_1=x$，$x_2=x^2$，$x_3=x^3$。所以$h(x)$就变成了
$$
h(x_1,x_2,x_3)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3
$$
然后可以用多元线性回归来解决这个问题。当然，这里所说的**复杂的函数**包含但不仅限于多项式函数。

## 8.正规方程

是一种可以代替梯度下降法的直接解法。问题抽象成：$X\theta=y$，$X_{m*n}$是拓展的特征向量矩阵(即第一列都是1)，$\theta$是参数向量，$y$是已知的结果。现在要求出使得$|y-X\theta|_2^2$最小的$\theta$。

$$
\begin{eqnarray}
\frac{d}{d\theta}|y-X\theta|_2^2 &=& \frac{d}{d\theta}(y-X\theta)^T(y-X\theta) \\
                                                           &=& -2X^T(y-X\theta)  \nonumber \\
                                                           &=& 0 \nonumber
\end{eqnarray}
$$

$$
\theta=(X^TX)^{-1}X^Ty
$$

如果$X$为方阵且有逆，就是我们熟悉的形式，$\theta=X^{-1}y$。

**经验告诉我们$n>10000$时倾向于使用梯度下降，反之则是正规方程。**因为当$n$太大时，$O(n^3)$求解$(X^TX)^{-1}$的速度太慢了。

## 9.正规方程不可逆时的解决办法

1. 多余的特征：例如有两个特征线性相关，可以删除掉其中一个特征。
2. 特征太多：删除掉某些特征，或使用正规化。

## 10.Logistic回归

适用于输出是0或1的二元分类问题。

### 10.1假设表示

$$
g(x)=\frac{1}{1+e^{-x}}
$$

$$
h_\theta(x)=g(\theta^Tx)
$$

式$(11)$被称为**sigmoid函数**或**Logistic函数**。其图像为

![2.png](基础分类和回归算法\2.png)

$h_\theta(x)$ = 输入为$x$输出为1的可能性。例如：$h_\theta(x_0)=0.7$，代表输入为$x_0$，输出有70%的可能性为1。数学表示为：$h_\theta(x)=P(y=1|x;\theta)$。同时易知
$$
P(y=1|x;\theta)+P(y=0|x;\theta)=1
$$

### 10.2决策界限

如果$P(y=1|x;\theta)\geq0.5$，我们预测$y=1$，反之$y=0$。所以
$$
y = \begin{cases}
1 & \theta^Tx \geq 0 \\
0 &  \theta^Tx < 0
\end{cases}
$$
所以空间将被$\theta^Tx$分为两个部分，$\theta^Tx$称为决策界限。通过训练集数据，我们可以拟合出决策界限。

### 10.3代价函数

如果将线性回归的代价函数作为现在的代价函数，那么
$$
J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2
$$

$$
h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}
$$

可以证明，$J(\theta)$不是凸函数，也就是有很多局部最优值。如果用梯度下降法很可能找不到全局最小值。所以我们需要寻找一个凸的代价函数。
$$
Cost(h_\theta(x),y)= \begin{cases}
-log(h_\theta(x)) & y=1 \\
-log(1-h_\theta(x)) & y=0
\end{cases}
$$

$$
J(\theta)=\frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)}),y)
$$

新的代价函数是一个凸函数。我们来简单理解一下$Cost(h_\theta(x),y)$这个函数。

当$y=1$时，$Cost(h_\theta(x),y)$图像为

![3.png](基础分类和回归算法\3.png)

可以看到，$y=1$且$x=1$($x$即$h_\theta(x)$)时，代价为0。$y=1$且$x=0$时，代价为正无穷大。这两种极端情况都是很合理的。

当$y=0$时，$Cost(h_\theta(x),y)$图像为

![4.png](基础分类和回归算法\4.png)

可以看到，$y=0$且$x=0$($x$即$h_\theta(x)$)时，代价为0。$y=0$且$x=1$时，代价为正无穷大。这两种极端情况也是很合理的。

### 10.4简化代价函数与梯度下降

代价函数可以简化为
$$
\begin{eqnarray}
J(\theta) &=& \frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)}),y^{(i)}) \\
               &=& -\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))] \nonumber
\end{eqnarray}
$$
梯度下降更新公式为
$$
\theta_j := \theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)
$$
又因为
$$
\begin{eqnarray}
\frac{\partial}{\partial\theta_j}J(\theta) &=& -\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\frac{1}{h_\theta(x^{(i)})}-(1-y^{(i)})\frac{1}{1-h_\theta(x^{(i)})}]\frac{\partial}{\partial\theta_j}h_\theta(x^{(i)}) \\
&=& -\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\frac{1}{h_\theta(x^{(i)})}-(1-y^{(i)})\frac{1}{1-h_\theta(x^{(i)})}]h_\theta(x^{(i)})(1-h_\theta(x^{(i)}))\theta_j \nonumber \\
&=& -\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}-h_\theta(x^{(i)})]x^{(i)}_j \nonumber \\
&=& \frac{1}{m}\sum_{i=1}^{m}[h_\theta(x^{(i)})-y^{(i)}]x^{(i)}_j \nonumber \\
\end{eqnarray}
$$

所以梯度下降更新公式可以写成
$$
\theta_j := \theta_j-\alpha\frac{1}{m}\sum_{i=1}^{m}[h_\theta(x^{(i)})-y^{(i)}]x^{(i)}_j
$$
因为$\alpha$为一不确定的常量，所以往往可以忽略$\frac{1}{m}$，即
$$
\theta_j := \theta_j-\alpha\sum_{i=1}^{m}[h_\theta(x^{(i)})-y^{(i)}]x^{(i)}_j
$$

### 10.5多类别分类

$n$类别分类问题，只需要拟合$n$个分类器即可。
$$
h_\theta^{(i)}(x)=P(y=i|x;\theta)
$$
最后分类的结果就是
$$
y=\max_i h_\theta^{(i)}(x)
$$

## 11.过拟合问题

**过拟合问题**：当特征过多时出现，拟合的函数千方百计的迎合训练集的数据，导致它无法泛化到新的样本中。

**泛化**：一个假设模型应用到新样本的能力。

**过拟合问题出现的原因**：特征太多，训练集数据太少。

**欠拟合问题**：拟合出的函数在训练集数据上表现很差。

**过拟合问题的解决方法**：

1. 删除掉一些特征
2. 模型选择算法
3. 正则化

## 12正则化

### 12.1代价函数

首先要明白“$\theta_i$**越小，模型就越简单**”。例如$\theta_0+\theta_1x_1+\theta_2x_2^2+\theta_3x_3^3$，如果$\theta_3$足够小，那么三阶项可以忽略，模型也就变成二阶的了。

受到上面的启发，我们写出新的代价函数。
$$
J(\theta)=\frac{1}{2m}[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^{n}\theta_j^2]
$$
新的代价函数加入了$\theta_1...\theta_n$的惩罚项(**正则化项**)$\lambda\sum_{j=1}^{n}\theta_j^2$。惩罚项的作用是让$\theta_0...\theta_n$尽可能的小。$\lambda$称为**正则化参数**。正则化参数用来控制两个目标的取舍，第一个目标是：训练出的函数$h$要更好的拟合训练集，第二个目标是：使参数$\theta_i$尽量的小。

### 12.2线性回归的正则化

#### 12.2.1梯度下降的正则化

$$
\theta_0 := \theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_0 \\
\theta_j := \theta_j-\alpha[\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j+\frac{\lambda}{m}\theta_j] (j=1,2,3,...,n)
$$

#### 12.2.2正规方程的正则化

我们的目标是最小化$J(\theta)=|y-X\theta)|_2^2+\lambda\sum_{j=1}^{n}\theta_j^2$，系数$\frac{1}{2m}$可以省略。
$$
\begin{eqnarray}
\frac{d}{d\theta}J(\theta) &=& -2X^T(y-X\theta)+2\lambda
\left[
\begin{matrix}
 0 \\
 \theta_1 \\
 \theta_2 \\
 \vdots \\
 \theta_n \\
\end{matrix}
\right] \\
                      &=& -2X^T(y-X\theta)+2\lambda

\left[
\begin{matrix}
 0 & 0 & \cdots & 0 \\
 0 & 1 & \cdots & 0 \\
 \vdots & \vdots & \ddots & \vdots \\
 0 & 0 & \cdots & 1 \\
\end{matrix}
\right]  \nonumber \\
                    &=& 0 \nonumber
\end{eqnarray}
$$
解得
$$
\theta=(X^TX+\lambda
\left[
\begin{matrix}
 0 & 0 & \cdots & 0 \\
 0 & 1 & \cdots & 0 \\
 \vdots & \vdots & \ddots & \vdots \\
 0 & 0 & \cdots & 1 \\
\end{matrix}
\right]
)^{-1}X^Ty
$$
可以证明
$$
X^TX+\lambda
\left[
\begin{matrix}
 0 & 0 & \cdots & 0 \\
 0 & 1 & \cdots & 0 \\
 \vdots & \vdots & \ddots & \vdots \\
 0 & 0 & \cdots & 1 \\
\end{matrix}
\right]
$$
是非奇异矩阵。

### 12.3Logistic回归的正则化

加上惩罚项的代价函数为
$$
J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j=1}^{m}\theta_j^2
$$
梯度下降的正则化更新公式为
$$
\theta_0 := \theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_0 \\
\theta_j := \theta_j-\alpha[\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j+\frac{\lambda}{m}\theta_j] (j=1,2,3,...,n)
$$
看起来和线性回归的完全一样，实际则不然。因为两者的$h_\theta$截然不同。
